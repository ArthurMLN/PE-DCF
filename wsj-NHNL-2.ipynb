{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5717f8fe-56b1-4176-8ba9-a7f56039784b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up WebDriver...\n",
      "WebDriver setup successful.\n",
      "Attempting to fetch URL: https://www.wsj.com/market-data/stocks/us\n",
      "Waiting up to 20 seconds for page elements to load...\n",
      "No cookie banner found or could not click (ID: onetrust-accept-btn-handler): Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0074FC03+61635]\n",
      "\tGetHandleVerifier [0x0074FC44+61700]\n",
      "\t(No symbol) [0x005705D3]\n",
      "\t(No symbol) [0x005B899E]\n",
      "\t(No symbol) [0x005B8D3B]\n",
      "\t(No symbol) [0x00600E12]\n",
      "\t(No symbol) [0x005DD2E4]\n",
      "\t(No symbol) [0x005FE61B]\n",
      "\t(No symbol) [0x005DD096]\n",
      "\t(No symbol) [0x005AC840]\n",
      "\t(No symbol) [0x005AD6A4]\n",
      "\tGetHandleVerifier [0x009D4523+2701795]\n",
      "\tGetHandleVerifier [0x009CFCA6+2683238]\n",
      "\tGetHandleVerifier [0x009EA9EE+2793134]\n",
      "\tGetHandleVerifier [0x007668C5+155013]\n",
      "\tGetHandleVerifier [0x0076CFAD+181357]\n",
      "\tGetHandleVerifier [0x00757458+92440]\n",
      "\tGetHandleVerifier [0x00757600+92864]\n",
      "\tGetHandleVerifier [0x00741FF0+5296]\n",
      "\tBaseThreadInitThunk [0x75935D49+25]\n",
      "\tRtlInitializeExceptionChain [0x7731D03B+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x7731CFC1+561]\n",
      "\n",
      "Main table content (e.g., 'Issues At New Highs') seems to have loaded.\n",
      "Markets Diary timestamp element seems to have loaded.\n",
      "Extracted Market Timestamp: 4:55 PM EDT 5/26/25\n",
      "Browser closed.\n",
      "\n",
      "--- Extracted Data (including Timestamp) ---\n",
      "Market Timestamp: 4:55 PM EDT 5/26/25\n",
      "  New Highs: Value 1 = 48, Value 2 = 70\n",
      "  New Lows: Value 1 = 47, Value 2 = 111\n",
      "  Advancing Issues: Value 1 = 1225, Value 2 = 1823\n",
      "  Declining Issues: Value 1 = 1537, Value 2 = 2671\n",
      "\n",
      "Data successfully saved to wsj_market_data.txt\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "\n",
    "# --- Configuration ---\n",
    "URL = \"https://www.wsj.com/market-data/stocks/us\"\n",
    "OUTPUT_FILENAME = \"wsj_market_data.txt\"\n",
    "MAX_WEBDRIVER_SETUP_ATTEMPTS = 3\n",
    "MAX_SCRAPING_ATTEMPTS = 3\n",
    "RETRY_DELAY_SECONDS = 5 # Delay between scraping retries\n",
    "WAIT_TIME_SECONDS = 20\n",
    "\n",
    "ITEMS_TO_SCRAPE = {\n",
    "    \"New Highs\": \"Issues At New Highs\",\n",
    "    \"New Lows\": \"Issues At New Lows\",\n",
    "    \"Advancing Issues\": \"Issues Advancing\",\n",
    "    \"Declining Issues\": \"Issues Declining\"\n",
    "}\n",
    "# --- End Configuration ---\n",
    "\n",
    "# --- Helper Function ---\n",
    "def extract_row_data(soup_obj, aria_label_value):\n",
    "    label_cell = soup_obj.find('td', attrs={'aria-label': aria_label_value})\n",
    "    if label_cell:\n",
    "        row = label_cell.find_parent('tr')\n",
    "        if row:\n",
    "            cells = row.find_all('td')\n",
    "            if len(cells) >= 3:\n",
    "                val1 = cells[1].get_text(strip=True).replace(',', '')\n",
    "                val2 = cells[2].get_text(strip=True).replace(',', '')\n",
    "                return {'value1': val1, 'value2': val2}\n",
    "    return None\n",
    "# --- End Helper Function ---\n",
    "\n",
    "# --- Main Script ---\n",
    "driver = None\n",
    "final_data = {} # Initialize to store final successfully scraped data\n",
    "\n",
    "# Setup Chrome options\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--disable-gpu')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\")\n",
    "\n",
    "try:\n",
    "    # --- WebDriver Setup with Retries ---\n",
    "    print(\"Setting up WebDriver...\")\n",
    "    for attempt in range(MAX_WEBDRIVER_SETUP_ATTEMPTS):\n",
    "        try:\n",
    "            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "            print(\"WebDriver setup successful.\")\n",
    "            break # Exit loop on successful setup\n",
    "        except Exception as e_wd:\n",
    "            print(f\"WebDriver setup attempt {attempt + 1}/{MAX_WEBDRIVER_SETUP_ATTEMPTS} failed: {e_wd}\")\n",
    "            if attempt < MAX_WEBDRIVER_SETUP_ATTEMPTS - 1:\n",
    "                print(f\"Retrying WebDriver setup in {RETRY_DELAY_SECONDS} seconds...\")\n",
    "                time.sleep(RETRY_DELAY_SECONDS)\n",
    "            else:\n",
    "                print(\"Max retries reached for WebDriver setup. Exiting.\")\n",
    "                raise # Re-raise the last exception if all attempts fail\n",
    "    if not driver:\n",
    "        print(\"Failed to initialize WebDriver. Exiting script.\")\n",
    "        exit()\n",
    "    # --- End WebDriver Setup ---\n",
    "\n",
    "    # --- Scraping Loop with Retries ---\n",
    "    for scrape_attempt in range(MAX_SCRAPING_ATTEMPTS):\n",
    "        print(f\"\\n--- Scraping Attempt {scrape_attempt + 1}/{MAX_SCRAPING_ATTEMPTS} ---\")\n",
    "        current_attempt_data = {} # Reset data for this attempt\n",
    "\n",
    "        try:\n",
    "            print(f\"Attempting to fetch URL: {URL}\")\n",
    "            driver.get(URL)\n",
    "\n",
    "            print(f\"Waiting up to {WAIT_TIME_SECONDS} seconds for page elements to load...\")\n",
    "\n",
    "            # --- Handle Cookie Banner ---\n",
    "            try:\n",
    "                cookie_button_id = 'onetrust-accept-btn-handler'\n",
    "                cookie_button = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.ID, cookie_button_id))\n",
    "                )\n",
    "                cookie_button.click()\n",
    "                print(\"Cookie banner accepted.\")\n",
    "                time.sleep(3) # Give time for banner to disappear\n",
    "            except Exception as e_cookie:\n",
    "                print(f\"No cookie banner found or could not click (ID: {cookie_button_id}): {e_cookie}\")\n",
    "            # --- End Cookie Handling ---\n",
    "\n",
    "            # --- Wait for Key Elements ---\n",
    "            key_table_element_xpath = f\"//td[@aria-label='{ITEMS_TO_SCRAPE['New Highs']}']\"\n",
    "            WebDriverWait(driver, WAIT_TIME_SECONDS).until(\n",
    "                EC.presence_of_element_located((By.XPATH, key_table_element_xpath))\n",
    "            )\n",
    "            print(f\"Main table content (e.g., '{ITEMS_TO_SCRAPE['New Highs']}') seems to have loaded.\")\n",
    "\n",
    "            diary_timestamp_xpath = \"//span[contains(@class, 'WSJBase--card__timestamp')]\"\n",
    "            WebDriverWait(driver, WAIT_TIME_SECONDS).until(\n",
    "                EC.presence_of_element_located((By.XPATH, diary_timestamp_xpath))\n",
    "            )\n",
    "            print(\"Markets Diary timestamp element seems to have loaded.\")\n",
    "            time.sleep(2) # Extra pause for full rendering\n",
    "            # --- End Wait for Key Elements ---\n",
    "\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "            # --- Extract Market Timestamp ---\n",
    "            market_timestamp_str = \"N/A\"\n",
    "            timestamp_span = soup.find('span', class_=lambda x: x and 'WSJBase--card__timestamp' in x) # More robust class search\n",
    "            if timestamp_span:\n",
    "                market_timestamp_str = timestamp_span.get_text(strip=True)\n",
    "                print(f\"Extracted Market Timestamp: {market_timestamp_str}\")\n",
    "            else:\n",
    "                print(\"Warning: Market timestamp span not found.\")\n",
    "            current_attempt_data['market_timestamp'] = market_timestamp_str\n",
    "            # --- End Timestamp Extraction ---\n",
    "\n",
    "            # --- Extract Table Data ---\n",
    "            table_data = {}\n",
    "            for display_name, aria_label in ITEMS_TO_SCRAPE.items():\n",
    "                row_data = extract_row_data(soup, aria_label)\n",
    "                if row_data:\n",
    "                    table_data[display_name] = row_data\n",
    "                else:\n",
    "                    print(f\"  Warning: Could not find or parse data for '{display_name}' (aria-label: '{aria_label}')\")\n",
    "            current_attempt_data['metrics'] = table_data\n",
    "            # --- End Table Data Extraction ---\n",
    "\n",
    "            # --- Check if data is sufficient ---\n",
    "            is_timestamp_valid = current_attempt_data.get('market_timestamp', \"N/A\") != \"N/A\"\n",
    "            are_metrics_present = bool(current_attempt_data.get('metrics'))\n",
    "\n",
    "            if is_timestamp_valid or are_metrics_present: # Consider valid if at least timestamp OR some metrics found\n",
    "                print(f\"Data successfully scraped on attempt {scrape_attempt + 1}.\")\n",
    "                final_data = current_attempt_data # Store successful data\n",
    "                break # Exit scraping loop on success\n",
    "            else:\n",
    "                print(f\"Scraping attempt {scrape_attempt + 1} yielded empty data.\")\n",
    "                if scrape_attempt < MAX_SCRAPING_ATTEMPTS - 1:\n",
    "                    print(f\"Retrying in {RETRY_DELAY_SECONDS} seconds...\")\n",
    "                    time.sleep(RETRY_DELAY_SECONDS)\n",
    "                else:\n",
    "                    print(\"Max scraping attempts reached. No valid data obtained.\")\n",
    "\n",
    "        except Exception as e_scrape:\n",
    "            print(f\"An error occurred during scraping attempt {scrape_attempt + 1}: {e}\")\n",
    "            try:\n",
    "                # Try to save page source only if driver is available\n",
    "                if driver:\n",
    "                    debug_filename = f\"wsj_error_page_attempt_{scrape_attempt + 1}.html\"\n",
    "                    with open(debug_filename, \"w\", encoding=\"utf-8\") as f_debug:\n",
    "                        f_debug.write(driver.page_source)\n",
    "                    print(f\"Saved error page source to {debug_filename}\")\n",
    "            except Exception as e_save_debug:\n",
    "                print(f\"Could not save debug page source: {e_save_debug}\")\n",
    "            \n",
    "            if scrape_attempt < MAX_SCRAPING_ATTEMPTS - 1:\n",
    "                print(f\"Retrying scraping in {RETRY_DELAY_SECONDS} seconds due to error...\")\n",
    "                time.sleep(RETRY_DELAY_SECONDS)\n",
    "            else:\n",
    "                print(\"Max scraping attempts reached after errors.\")\n",
    "        # No driver.quit() here, we reuse it for the next attempt or quit it in the outer finally\n",
    "\n",
    "    # --- End Scraping Loop ---\n",
    "\n",
    "except Exception as e_outer:\n",
    "    print(f\"An critical error occurred outside the scraping loop: {e_outer}\")\n",
    "\n",
    "finally:\n",
    "    if driver:\n",
    "        driver.quit()\n",
    "        print(\"Browser closed.\")\n",
    "\n",
    "# --- Process and Save Final Data ---\n",
    "print(\"\\n--- Final Extracted Data ---\")\n",
    "if final_data.get('market_timestamp', 'N/A') != \"N/A\" or final_data.get('metrics'):\n",
    "    print(f\"Market Timestamp: {final_data.get('market_timestamp', 'N/A')}\")\n",
    "    if final_data.get('metrics'):\n",
    "        for key, values in final_data['metrics'].items():\n",
    "            print(f\"  {key}: Value 1 = {values['value1']}, Value 2 = {values['value2']}\")\n",
    "\n",
    "    try:\n",
    "        with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as f_out:\n",
    "            json.dump(final_data, f_out, indent=4, ensure_ascii=False)\n",
    "        print(f\"\\nData successfully saved to {OUTPUT_FILENAME}\")\n",
    "    except IOError as e_io:\n",
    "        print(f\"Error saving data to file {OUTPUT_FILENAME}: {e_io}\")\n",
    "    except Exception as e_json:\n",
    "        print(f\"Error during JSON serialization: {e_json}\")\n",
    "else:\n",
    "    print(\"No significant data was extracted after all attempts. Nothing to save to file.\")\n",
    "\n",
    "print(\"Script finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71047a61-3a7a-402b-af73-be0ec6af7b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_data_from_file(filepath=\"wsj_market_data.txt\"):\n",
    "    \"\"\"\n",
    "    从指定的 TXT 文件加载 JSON 数据。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f_in:\n",
    "            data = json.load(f_in)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filepath}' not found.\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Could not decode JSON from '{filepath}'. The file might be corrupted or not in JSON format.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while reading '{filepath}': {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    market_data_filepath = \"wsj_market_data.txt\"\n",
    "    loaded_data = load_data_from_file(market_data_filepath)\n",
    "\n",
    "    if loaded_data:\n",
    "        print(\"--- Successfully Loaded Market Data ---\")\n",
    "        \n",
    "        # 读取并打印市场时间戳\n",
    "        market_time = loaded_data.get('market_timestamp', 'Timestamp not found in data')\n",
    "        print(f\"Market Timestamp: {market_time}\")\n",
    "\n",
    "        # 读取并打印指标数据 (if you used the nested structure)\n",
    "        metrics_data = loaded_data.get('metrics')\n",
    "        if metrics_data:\n",
    "            print(\"\\nMetrics:\")\n",
    "            for key, values in metrics_data.items():\n",
    "                print(f\"  {key}: Value 1 = {values.get('value1', 'N/A')}, Value 2 = {values.get('value2', 'N/A')}\")\n",
    "        else:\n",
    "            print(\"No metrics data found.\")\n",
    "            # If you didn't nest 'metrics', you'd iterate directly over loaded_data for those keys,\n",
    "            # skipping 'market_timestamp'. Example for flat structure:\n",
    "            # print(\"\\nMetrics:\")\n",
    "            # for key, values in loaded_data.items():\n",
    "            #     if key != 'market_timestamp' and isinstance(values, dict): # Check if it's metric data\n",
    "            #         print(f\"  {key}: Value 1 = {values.get('value1', 'N/A')}, Value 2 = {values.get('value2', 'N/A')}\")\n",
    "\n",
    "\n",
    "        # Example of accessing specific data:\n",
    "        # if metrics_data and \"New Highs\" in metrics_data:\n",
    "        #     new_highs_val1 = metrics_data[\"New Highs\"].get(\"value1\")\n",
    "        #     print(f\"\\nSpecific access - New Highs Value 1: {new_highs_val1}\")\n",
    "            \n",
    "    else:\n",
    "        print(\"Failed to load data from the file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8525dc88-e89c-4cce-8be6-344c56fae56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yz/jupyter_env/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'webdriver_manager'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mby\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m By\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchrome\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mservice\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Service\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwebdriver_manager\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchrome\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChromeDriverManager\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msupport\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mui\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WebDriverWait\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msupport\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m expected_conditions \u001b[38;5;28;01mas\u001b[39;00m EC\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'webdriver_manager'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "\n",
    "# --- Global Settings ---\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY_SECONDS = 10\n",
    "OUTPUT_FILENAME = \"cnn_fear_greed_data.txt\" # Using .json for clarity\n",
    "\n",
    "# Chrome options (defined once)\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--disable-gpu')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\")\n",
    "# options.add_argument(\"--window-size=1920,1080\")\n",
    "\n",
    "def scrape_cnn_data(attempt_num):\n",
    "    \"\"\"\n",
    "    Attempts to scrape CNN Fear & Greed data.\n",
    "    Returns a tuple (data_dict, success_bool).\n",
    "    Manages its own WebDriver instance.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Scraping Attempt {attempt_num} of {MAX_RETRIES} ---\")\n",
    "    local_driver = None\n",
    "    scraped_data = {}\n",
    "    attempt_success = False\n",
    "\n",
    "    try:\n",
    "        print(\"Setting up WebDriver for this attempt...\")\n",
    "        for wd_setup_attempt in range(3): # WebDriver setup retry loop\n",
    "            try:\n",
    "                local_driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "                print(\"WebDriver setup successful for this attempt.\")\n",
    "                break\n",
    "            except Exception as e_wd:\n",
    "                print(f\"WebDriver setup (inner) attempt {wd_setup_attempt + 1} failed: {e_wd}\")\n",
    "                if wd_setup_attempt < 2:\n",
    "                    print(\"Retrying WebDriver setup in 5 seconds...\")\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    print(\"Max retries reached for WebDriver setup for this attempt.\")\n",
    "                    # This exception will be caught by the outer try-except of scrape_cnn_data\n",
    "                    raise RuntimeError(\"WebDriver setup failed after multiple retries.\")\n",
    "\n",
    "        if not local_driver:\n",
    "            print(\"Failed to initialize WebDriver for this attempt. This attempt will fail.\")\n",
    "            return {}, False\n",
    "\n",
    "        url = \"https://www.cnn.com/markets/fear-and-greed\"\n",
    "        print(f\"Attempting to fetch URL: {url}\")\n",
    "        local_driver.get(url)\n",
    "\n",
    "        # Increased wait time as CNN can be slow and has dynamic content\n",
    "        wait_time = 25\n",
    "        print(f\"Waiting up to {wait_time} seconds for page elements to load...\")\n",
    "\n",
    "        # --- Optional: Cookie Banner Handling ---\n",
    "        # CNN's cookie banner can be tricky. This is a best-effort attempt.\n",
    "        # If it fails or is not needed, scraping might still proceed.\n",
    "        try:\n",
    "            # Common cookie banner button ID, but might change or be in shadow DOM\n",
    "            cookie_button_id = 'onetrust-accept-btn-handler'\n",
    "            cookie_button_xpath = f\"//button[@id='{cookie_button_id}']\" # More robust locator\n",
    "\n",
    "            # Sometimes the banner is within an iframe or shadow DOM,\n",
    "            # making direct clicks hard. If this simple click fails,\n",
    "            # the script will continue, hoping the banner doesn't obstruct.\n",
    "            WebDriverWait(local_driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, cookie_button_xpath))\n",
    "            ).click()\n",
    "            print(\"Cookie banner likely accepted.\")\n",
    "            time.sleep(3)  # Give time for banner to disappear and page to settle\n",
    "        except Exception as e_cookie:\n",
    "            print(f\"Could not click cookie banner (ID: {cookie_button_id}): {e_cookie}. Continuing...\")\n",
    "            # Try to scroll down a bit if banner is an overlay\n",
    "            try:\n",
    "                local_driver.execute_script(\"window.scrollBy(0, 200);\")\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "        # Wait for the specific element containing the Fear & Greed value\n",
    "        target_value_class = \"market-fng-gauge__dial-number-value\"\n",
    "        WebDriverWait(local_driver, wait_time).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, target_value_class))\n",
    "        )\n",
    "        # Also wait for the timestamp element\n",
    "        timestamp_class = \"market-fng-gauge__timestamp\"\n",
    "        WebDriverWait(local_driver, wait_time).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, timestamp_class))\n",
    "        )\n",
    "        print(\"Key elements (value and timestamp) seem to have loaded.\")\n",
    "        time.sleep(3) # Extra pause for full rendering\n",
    "\n",
    "        page_source = local_driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # --- Extract Fear & Greed Index Value ---\n",
    "        fear_greed_index_str = \"N/A\"\n",
    "        value_span = soup.find('span', class_=target_value_class)\n",
    "        if value_span:\n",
    "            fear_greed_index_str = value_span.get_text(strip=True)\n",
    "            print(f\"Extracted Fear & Greed Index String: {fear_greed_index_str}\")\n",
    "            try:\n",
    "                scraped_data['fear_greed_index'] = int(fear_greed_index_str)\n",
    "            except ValueError:\n",
    "                scraped_data['fear_greed_index'] = fear_greed_index_str\n",
    "                print(f\"Warning: Could not convert Fear & Greed Index '{fear_greed_index_str}' to an integer.\")\n",
    "        else:\n",
    "            print(f\"Warning: Fear & Greed index value span not found using class '{target_value_class}'.\")\n",
    "            scraped_data['fear_greed_index'] = \"N/A\"\n",
    "\n",
    "        # --- Extract Market Timestamp ---\n",
    "        market_timestamp_str = \"N/A\"\n",
    "        timestamp_div = soup.find('div', class_=timestamp_class)\n",
    "        if timestamp_div:\n",
    "            market_timestamp_str = timestamp_div.get_text(strip=True)\n",
    "            if \"Last updated \" in market_timestamp_str: # Clean up the string\n",
    "                market_timestamp_str = market_timestamp_str.replace(\"Last updated \", \"\").strip()\n",
    "            print(f\"Extracted Market Timestamp: {market_timestamp_str}\")\n",
    "        else:\n",
    "            print(f\"Warning: Market timestamp div not found using class '{timestamp_class}'.\")\n",
    "        scraped_data['market_timestamp'] = market_timestamp_str\n",
    "\n",
    "        # --- Determine if this attempt was successful ---\n",
    "        if scraped_data.get('fear_greed_index', \"N/A\") != \"N/A\" and \\\n",
    "           scraped_data.get('market_timestamp', \"N/A\") != \"N/A\":\n",
    "            print(\"Data extraction successful for this attempt.\")\n",
    "            attempt_success = True\n",
    "        else:\n",
    "            print(\"Data extraction was incomplete for this attempt.\")\n",
    "            # Save page source for debugging if extraction failed but page seemed to load\n",
    "            debug_filename = f\"cnn_extraction_fail_debug_attempt_{attempt_num}.html\"\n",
    "            try:\n",
    "                with open(debug_filename, \"w\", encoding=\"utf-8\") as f_debug:\n",
    "                    f_debug.write(local_driver.page_source)\n",
    "                print(f\"Saved incomplete extraction page source to {debug_filename}\")\n",
    "            except Exception as e_save:\n",
    "                print(f\"Could not save debug page source for incomplete extraction: {e_save}\")\n",
    "\n",
    "        return scraped_data, attempt_success\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during scraping attempt {attempt_num}: {e}\")\n",
    "        if local_driver: # If driver was initialized, try to save page source\n",
    "            try:\n",
    "                # Save page source for debugging if an error occurs\n",
    "                debug_filename = f\"cnn_error_page_debug_attempt_{attempt_num}.html\"\n",
    "                with open(debug_filename, \"w\", encoding=\"utf-8\") as f_debug:\n",
    "                    f_debug.write(local_driver.page_source)\n",
    "                print(f\"Saved error page source to {debug_filename}\")\n",
    "            except Exception as e_save:\n",
    "                print(f\"Could not save debug page source on error: {e_save}\")\n",
    "        return {}, False # Return empty data and failure for this attempt\n",
    "    finally:\n",
    "        if local_driver:\n",
    "            local_driver.quit()\n",
    "            print(f\"Browser closed for attempt {attempt_num}.\")\n",
    "\n",
    "\n",
    "# --- Main Execution Logic with Retries ---\n",
    "if __name__ == \"__main__\":\n",
    "    final_extracted_data = {}\n",
    "    overall_success = False\n",
    "\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        data_from_attempt, success_status = scrape_cnn_data(attempt)\n",
    "        if success_status:\n",
    "            final_extracted_data = data_from_attempt\n",
    "            overall_success = True\n",
    "            print(f\"\\nSuccessfully fetched data on attempt {attempt}.\")\n",
    "            break # Exit loop on success\n",
    "        elif attempt < MAX_RETRIES:\n",
    "            print(f\"Attempt {attempt} failed. Retrying in {RETRY_DELAY_SECONDS} seconds...\")\n",
    "            time.sleep(RETRY_DELAY_SECONDS)\n",
    "        else:\n",
    "            print(\"\\nAll scraping attempts failed.\")\n",
    "\n",
    "    print(\"\\n--- Final Extracted Data ---\")\n",
    "    if overall_success and final_extracted_data:\n",
    "        print(f\"Market Timestamp: {final_extracted_data.get('market_timestamp', 'N/A')}\")\n",
    "        print(f\"Fear & Greed Index: {final_extracted_data.get('fear_greed_index', 'N/A')}\")\n",
    "\n",
    "        try:\n",
    "            with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as f_out:\n",
    "                json.dump(final_extracted_data, f_out, indent=4, ensure_ascii=False)\n",
    "            print(f\"\\nData successfully saved to {OUTPUT_FILENAME}\")\n",
    "        except IOError as e_io:\n",
    "            print(f\"Error saving data to file {OUTPUT_FILENAME}: {e_io}\")\n",
    "        except Exception as e_json:\n",
    "            print(f\"Error during JSON serialization or file writing: {e_json}\")\n",
    "    else:\n",
    "        print(\"No significant data was extracted after all retries. Nothing to save to file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e8d7b6-c74e-4807-af7f-25a000ec029e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
