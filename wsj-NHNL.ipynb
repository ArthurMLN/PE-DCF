{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5717f8fe-56b1-4176-8ba9-a7f56039784b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up WebDriver...\n",
      "WebDriver setup successful.\n",
      "Attempting to fetch URL: https://www.wsj.com/market-data/stocks/us\n",
      "Waiting up to 20 seconds for page elements to load...\n",
      "No cookie banner found or could not click (ID: onetrust-accept-btn-handler): Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x0074FC03+61635]\n",
      "\tGetHandleVerifier [0x0074FC44+61700]\n",
      "\t(No symbol) [0x005705D3]\n",
      "\t(No symbol) [0x005B899E]\n",
      "\t(No symbol) [0x005B8D3B]\n",
      "\t(No symbol) [0x00600E12]\n",
      "\t(No symbol) [0x005DD2E4]\n",
      "\t(No symbol) [0x005FE61B]\n",
      "\t(No symbol) [0x005DD096]\n",
      "\t(No symbol) [0x005AC840]\n",
      "\t(No symbol) [0x005AD6A4]\n",
      "\tGetHandleVerifier [0x009D4523+2701795]\n",
      "\tGetHandleVerifier [0x009CFCA6+2683238]\n",
      "\tGetHandleVerifier [0x009EA9EE+2793134]\n",
      "\tGetHandleVerifier [0x007668C5+155013]\n",
      "\tGetHandleVerifier [0x0076CFAD+181357]\n",
      "\tGetHandleVerifier [0x00757458+92440]\n",
      "\tGetHandleVerifier [0x00757600+92864]\n",
      "\tGetHandleVerifier [0x00741FF0+5296]\n",
      "\tBaseThreadInitThunk [0x75935D49+25]\n",
      "\tRtlInitializeExceptionChain [0x7731D03B+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x7731CFC1+561]\n",
      "\n",
      "Main table content (e.g., 'Issues At New Highs') seems to have loaded.\n",
      "Markets Diary timestamp element seems to have loaded.\n",
      "Extracted Market Timestamp: 4:55 PM EDT 5/26/25\n",
      "Browser closed.\n",
      "\n",
      "--- Extracted Data (including Timestamp) ---\n",
      "Market Timestamp: 4:55 PM EDT 5/26/25\n",
      "  New Highs: Value 1 = 48, Value 2 = 70\n",
      "  New Lows: Value 1 = 47, Value 2 = 111\n",
      "  Advancing Issues: Value 1 = 1225, Value 2 = 1823\n",
      "  Declining Issues: Value 1 = 1537, Value 2 = 2671\n",
      "\n",
      "Data successfully saved to wsj_market_data.txt\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "\n",
    "# --- Configuration ---\n",
    "URL = \"https://www.wsj.com/market-data/stocks/us\"\n",
    "OUTPUT_FILENAME = \"wsj_market_data.txt\"\n",
    "MAX_WEBDRIVER_SETUP_ATTEMPTS = 3\n",
    "MAX_SCRAPING_ATTEMPTS = 3\n",
    "RETRY_DELAY_SECONDS = 5 # Delay between scraping retries\n",
    "WAIT_TIME_SECONDS = 20\n",
    "\n",
    "ITEMS_TO_SCRAPE = {\n",
    "    \"New Highs\": \"Issues At New Highs\",\n",
    "    \"New Lows\": \"Issues At New Lows\",\n",
    "    \"Advancing Issues\": \"Issues Advancing\",\n",
    "    \"Declining Issues\": \"Issues Declining\"\n",
    "}\n",
    "# --- End Configuration ---\n",
    "\n",
    "# --- Helper Function ---\n",
    "def extract_row_data(soup_obj, aria_label_value):\n",
    "    label_cell = soup_obj.find('td', attrs={'aria-label': aria_label_value})\n",
    "    if label_cell:\n",
    "        row = label_cell.find_parent('tr')\n",
    "        if row:\n",
    "            cells = row.find_all('td')\n",
    "            if len(cells) >= 3:\n",
    "                val1 = cells[1].get_text(strip=True).replace(',', '')\n",
    "                val2 = cells[2].get_text(strip=True).replace(',', '')\n",
    "                return {'value1': val1, 'value2': val2}\n",
    "    return None\n",
    "# --- End Helper Function ---\n",
    "\n",
    "# --- Main Script ---\n",
    "driver = None\n",
    "final_data = {} # Initialize to store final successfully scraped data\n",
    "\n",
    "# Setup Chrome options\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--disable-gpu')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\")\n",
    "\n",
    "try:\n",
    "    # --- WebDriver Setup with Retries ---\n",
    "    print(\"Setting up WebDriver...\")\n",
    "    for attempt in range(MAX_WEBDRIVER_SETUP_ATTEMPTS):\n",
    "        try:\n",
    "            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "            print(\"WebDriver setup successful.\")\n",
    "            break # Exit loop on successful setup\n",
    "        except Exception as e_wd:\n",
    "            print(f\"WebDriver setup attempt {attempt + 1}/{MAX_WEBDRIVER_SETUP_ATTEMPTS} failed: {e_wd}\")\n",
    "            if attempt < MAX_WEBDRIVER_SETUP_ATTEMPTS - 1:\n",
    "                print(f\"Retrying WebDriver setup in {RETRY_DELAY_SECONDS} seconds...\")\n",
    "                time.sleep(RETRY_DELAY_SECONDS)\n",
    "            else:\n",
    "                print(\"Max retries reached for WebDriver setup. Exiting.\")\n",
    "                raise # Re-raise the last exception if all attempts fail\n",
    "    if not driver:\n",
    "        print(\"Failed to initialize WebDriver. Exiting script.\")\n",
    "        exit()\n",
    "    # --- End WebDriver Setup ---\n",
    "\n",
    "    # --- Scraping Loop with Retries ---\n",
    "    for scrape_attempt in range(MAX_SCRAPING_ATTEMPTS):\n",
    "        print(f\"\\n--- Scraping Attempt {scrape_attempt + 1}/{MAX_SCRAPING_ATTEMPTS} ---\")\n",
    "        current_attempt_data = {} # Reset data for this attempt\n",
    "\n",
    "        try:\n",
    "            print(f\"Attempting to fetch URL: {URL}\")\n",
    "            driver.get(URL)\n",
    "\n",
    "            print(f\"Waiting up to {WAIT_TIME_SECONDS} seconds for page elements to load...\")\n",
    "\n",
    "            # --- Handle Cookie Banner ---\n",
    "            try:\n",
    "                cookie_button_id = 'onetrust-accept-btn-handler'\n",
    "                cookie_button = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.ID, cookie_button_id))\n",
    "                )\n",
    "                cookie_button.click()\n",
    "                print(\"Cookie banner accepted.\")\n",
    "                time.sleep(3) # Give time for banner to disappear\n",
    "            except Exception as e_cookie:\n",
    "                print(f\"No cookie banner found or could not click (ID: {cookie_button_id}): {e_cookie}\")\n",
    "            # --- End Cookie Handling ---\n",
    "\n",
    "            # --- Wait for Key Elements ---\n",
    "            key_table_element_xpath = f\"//td[@aria-label='{ITEMS_TO_SCRAPE['New Highs']}']\"\n",
    "            WebDriverWait(driver, WAIT_TIME_SECONDS).until(\n",
    "                EC.presence_of_element_located((By.XPATH, key_table_element_xpath))\n",
    "            )\n",
    "            print(f\"Main table content (e.g., '{ITEMS_TO_SCRAPE['New Highs']}') seems to have loaded.\")\n",
    "\n",
    "            diary_timestamp_xpath = \"//span[contains(@class, 'WSJBase--card__timestamp')]\"\n",
    "            WebDriverWait(driver, WAIT_TIME_SECONDS).until(\n",
    "                EC.presence_of_element_located((By.XPATH, diary_timestamp_xpath))\n",
    "            )\n",
    "            print(\"Markets Diary timestamp element seems to have loaded.\")\n",
    "            time.sleep(2) # Extra pause for full rendering\n",
    "            # --- End Wait for Key Elements ---\n",
    "\n",
    "            page_source = driver.page_source\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "            # --- Extract Market Timestamp ---\n",
    "            market_timestamp_str = \"N/A\"\n",
    "            timestamp_span = soup.find('span', class_=lambda x: x and 'WSJBase--card__timestamp' in x) # More robust class search\n",
    "            if timestamp_span:\n",
    "                market_timestamp_str = timestamp_span.get_text(strip=True)\n",
    "                print(f\"Extracted Market Timestamp: {market_timestamp_str}\")\n",
    "            else:\n",
    "                print(\"Warning: Market timestamp span not found.\")\n",
    "            current_attempt_data['market_timestamp'] = market_timestamp_str\n",
    "            # --- End Timestamp Extraction ---\n",
    "\n",
    "            # --- Extract Table Data ---\n",
    "            table_data = {}\n",
    "            for display_name, aria_label in ITEMS_TO_SCRAPE.items():\n",
    "                row_data = extract_row_data(soup, aria_label)\n",
    "                if row_data:\n",
    "                    table_data[display_name] = row_data\n",
    "                else:\n",
    "                    print(f\"  Warning: Could not find or parse data for '{display_name}' (aria-label: '{aria_label}')\")\n",
    "            current_attempt_data['metrics'] = table_data\n",
    "            # --- End Table Data Extraction ---\n",
    "\n",
    "            # --- Check if data is sufficient ---\n",
    "            is_timestamp_valid = current_attempt_data.get('market_timestamp', \"N/A\") != \"N/A\"\n",
    "            are_metrics_present = bool(current_attempt_data.get('metrics'))\n",
    "\n",
    "            if is_timestamp_valid or are_metrics_present: # Consider valid if at least timestamp OR some metrics found\n",
    "                print(f\"Data successfully scraped on attempt {scrape_attempt + 1}.\")\n",
    "                final_data = current_attempt_data # Store successful data\n",
    "                break # Exit scraping loop on success\n",
    "            else:\n",
    "                print(f\"Scraping attempt {scrape_attempt + 1} yielded empty data.\")\n",
    "                if scrape_attempt < MAX_SCRAPING_ATTEMPTS - 1:\n",
    "                    print(f\"Retrying in {RETRY_DELAY_SECONDS} seconds...\")\n",
    "                    time.sleep(RETRY_DELAY_SECONDS)\n",
    "                else:\n",
    "                    print(\"Max scraping attempts reached. No valid data obtained.\")\n",
    "\n",
    "        except Exception as e_scrape:\n",
    "            print(f\"An error occurred during scraping attempt {scrape_attempt + 1}: {e}\")\n",
    "            try:\n",
    "                # Try to save page source only if driver is available\n",
    "                if driver:\n",
    "                    debug_filename = f\"wsj_error_page_attempt_{scrape_attempt + 1}.html\"\n",
    "                    with open(debug_filename, \"w\", encoding=\"utf-8\") as f_debug:\n",
    "                        f_debug.write(driver.page_source)\n",
    "                    print(f\"Saved error page source to {debug_filename}\")\n",
    "            except Exception as e_save_debug:\n",
    "                print(f\"Could not save debug page source: {e_save_debug}\")\n",
    "            \n",
    "            if scrape_attempt < MAX_SCRAPING_ATTEMPTS - 1:\n",
    "                print(f\"Retrying scraping in {RETRY_DELAY_SECONDS} seconds due to error...\")\n",
    "                time.sleep(RETRY_DELAY_SECONDS)\n",
    "            else:\n",
    "                print(\"Max scraping attempts reached after errors.\")\n",
    "        # No driver.quit() here, we reuse it for the next attempt or quit it in the outer finally\n",
    "\n",
    "    # --- End Scraping Loop ---\n",
    "\n",
    "except Exception as e_outer:\n",
    "    print(f\"An critical error occurred outside the scraping loop: {e_outer}\")\n",
    "\n",
    "finally:\n",
    "    if driver:\n",
    "        driver.quit()\n",
    "        print(\"Browser closed.\")\n",
    "\n",
    "# --- Process and Save Final Data ---\n",
    "print(\"\\n--- Final Extracted Data ---\")\n",
    "if final_data.get('market_timestamp', 'N/A') != \"N/A\" or final_data.get('metrics'):\n",
    "    print(f\"Market Timestamp: {final_data.get('market_timestamp', 'N/A')}\")\n",
    "    if final_data.get('metrics'):\n",
    "        for key, values in final_data['metrics'].items():\n",
    "            print(f\"  {key}: Value 1 = {values['value1']}, Value 2 = {values['value2']}\")\n",
    "\n",
    "    try:\n",
    "        with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as f_out:\n",
    "            json.dump(final_data, f_out, indent=4, ensure_ascii=False)\n",
    "        print(f\"\\nData successfully saved to {OUTPUT_FILENAME}\")\n",
    "    except IOError as e_io:\n",
    "        print(f\"Error saving data to file {OUTPUT_FILENAME}: {e_io}\")\n",
    "    except Exception as e_json:\n",
    "        print(f\"Error during JSON serialization: {e_json}\")\n",
    "else:\n",
    "    print(\"No significant data was extracted after all attempts. Nothing to save to file.\")\n",
    "\n",
    "print(\"Script finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71047a61-3a7a-402b-af73-be0ec6af7b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def load_data_from_file(filepath=\"wsj_market_data.txt\"):\n",
    "    \"\"\"\n",
    "    从指定的 TXT 文件加载 JSON 数据。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f_in:\n",
    "            data = json.load(f_in)\n",
    "        return data\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filepath}' not found.\")\n",
    "        return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Could not decode JSON from '{filepath}'. The file might be corrupted or not in JSON format.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while reading '{filepath}': {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    market_data_filepath = \"wsj_market_data.txt\"\n",
    "    loaded_data = load_data_from_file(market_data_filepath)\n",
    "\n",
    "    if loaded_data:\n",
    "        print(\"--- Successfully Loaded Market Data ---\")\n",
    "        \n",
    "        # 读取并打印市场时间戳\n",
    "        market_time = loaded_data.get('market_timestamp', 'Timestamp not found in data')\n",
    "        print(f\"Market Timestamp: {market_time}\")\n",
    "\n",
    "        # 读取并打印指标数据 (if you used the nested structure)\n",
    "        metrics_data = loaded_data.get('metrics')\n",
    "        if metrics_data:\n",
    "            print(\"\\nMetrics:\")\n",
    "            for key, values in metrics_data.items():\n",
    "                print(f\"  {key}: Value 1 = {values.get('value1', 'N/A')}, Value 2 = {values.get('value2', 'N/A')}\")\n",
    "        else:\n",
    "            print(\"No metrics data found.\")\n",
    "            # If you didn't nest 'metrics', you'd iterate directly over loaded_data for those keys,\n",
    "            # skipping 'market_timestamp'. Example for flat structure:\n",
    "            # print(\"\\nMetrics:\")\n",
    "            # for key, values in loaded_data.items():\n",
    "            #     if key != 'market_timestamp' and isinstance(values, dict): # Check if it's metric data\n",
    "            #         print(f\"  {key}: Value 1 = {values.get('value1', 'N/A')}, Value 2 = {values.get('value2', 'N/A')}\")\n",
    "\n",
    "\n",
    "        # Example of accessing specific data:\n",
    "        # if metrics_data and \"New Highs\" in metrics_data:\n",
    "        #     new_highs_val1 = metrics_data[\"New Highs\"].get(\"value1\")\n",
    "        #     print(f\"\\nSpecific access - New Highs Value 1: {new_highs_val1}\")\n",
    "            \n",
    "    else:\n",
    "        print(\"Failed to load data from the file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8525dc88-e89c-4cce-8be6-344c56fae56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up WebDriver...\n",
      "WebDriver setup successful.\n",
      "Attempting to fetch URL: https://www.cnn.com/markets/fear-and-greed\n",
      "Waiting up to 30 seconds for page elements to load...\n",
      "Could not click cookie banner by ID 'onetrust-accept-btn-handler': Message: \n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00ADFC03+61635]\n",
      "\tGetHandleVerifier [0x00ADFC44+61700]\n",
      "\t(No symbol) [0x009005D3]\n",
      "\t(No symbol) [0x0094899E]\n",
      "\t(No symbol) [0x00948D3B]\n",
      "\t(No symbol) [0x00990E12]\n",
      "\t(No symbol) [0x0096D2E4]\n",
      "\t(No symbol) [0x0098E61B]\n",
      "\t(No symbol) [0x0096D096]\n",
      "\t(No symbol) [0x0093C840]\n",
      "\t(No symbol) [0x0093D6A4]\n",
      "\tGetHandleVerifier [0x00D64523+2701795]\n",
      "\tGetHandleVerifier [0x00D5FCA6+2683238]\n",
      "\tGetHandleVerifier [0x00D7A9EE+2793134]\n",
      "\tGetHandleVerifier [0x00AF68C5+155013]\n",
      "\tGetHandleVerifier [0x00AFCFAD+181357]\n",
      "\tGetHandleVerifier [0x00AE7458+92440]\n",
      "\tGetHandleVerifier [0x00AE7600+92864]\n",
      "\tGetHandleVerifier [0x00AD1FF0+5296]\n",
      "\tBaseThreadInitThunk [0x75935D49+25]\n",
      "\tRtlInitializeExceptionChain [0x7731D03B+107]\n",
      "\tRtlGetAppContainerNamedObjectPath [0x7731CFC1+561]\n",
      "\n",
      "Continuing without explicit cookie banner click, hoping it doesn't interfere.\n",
      "Fear & Greed index value element (class: 'market-fng-gauge__dial-number-value') seems to have loaded.\n",
      "Fear & Greed timestamp element (class: 'market-fng-gauge__timestamp') seems to have loaded.\n",
      "Extracted Fear & Greed Index String: 64\n",
      "Extracted Market Timestamp: May 23 at 7:59:41 PM ET\n",
      "Browser closed.\n",
      "\n",
      "--- Extracted Data ---\n",
      "Market Timestamp: May 23 at 7:59:41 PM ET\n",
      "Fear & Greed Index: 64\n",
      "\n",
      "Data successfully saved to cnn_fear_greed_data.txt\n"
     ]
    }
   ],
   "source": [
    "#贪婪恐惧指数cnn\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json # 导入 json 模块\n",
    "\n",
    "# 设置 Chrome 选项\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')\n",
    "options.add_argument('--disable-gpu')\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--disable-dev-shm-usage')\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\")\n",
    "# options.add_argument(\"--window-size=1920,1080\") # 有时无头模式下需要指定窗口大小\n",
    "\n",
    "driver = None\n",
    "data = {} # 初始化数据字典\n",
    "output_filename = \"cnn_fear_greed_data.txt\" # 修改输出文件名\n",
    "\n",
    "# 注意：CNN页面结构相对简单，可能不需要复杂的extract_row_data函数，直接在主逻辑中提取\n",
    "\n",
    "try:\n",
    "    print(\"Setting up WebDriver...\")\n",
    "    for attempt in range(3):\n",
    "        try:\n",
    "            driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "            print(\"WebDriver setup successful.\")\n",
    "            break\n",
    "        except Exception as e_wd:\n",
    "            print(f\"WebDriver setup attempt {attempt + 1} failed: {e_wd}\")\n",
    "            if attempt < 2:\n",
    "                print(\"Retrying in 5 seconds...\")\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                print(\"Max retries reached for WebDriver setup. Exiting.\")\n",
    "                raise\n",
    "\n",
    "    if not driver:\n",
    "        print(\"Failed to initialize WebDriver after multiple attempts.\")\n",
    "        exit()\n",
    "\n",
    "    url = \"https://www.cnn.com/markets/fear-and-greed\" # 修改URL\n",
    "    print(f\"Attempting to fetch URL: {url}\")\n",
    "    driver.get(url)\n",
    "\n",
    "    wait_time = 10 # 增加等待时间，CNN页面有时加载较慢\n",
    "    print(f\"Waiting up to {wait_time} seconds for page elements to load...\")\n",
    "\n",
    "    # --- 尝试处理 Cookie 横幅 (CNN的Cookie横幅ID可能不同，需要检查) ---\n",
    "    # CNN Cookie banner might be different. Common ones are related to 'onetrust'\n",
    "    # Inspect the page to find the correct ID or class if this doesn't work.\n",
    "    # Sometimes it's a button within a div with id 'onetrust-banner-sdk'\n",
    "    # For CNN, it seems to be a button inside a shadow DOM or a more complex structure.\n",
    "    # Let's try a general approach if a simple ID click fails.\n",
    "    # Often, the cookie banner might not interfere with headless scraping of main content.\n",
    "    # If it does, more advanced handling (like executing JavaScript to click) might be needed.\n",
    "    # For now, we'll keep a simple attempt.\n",
    "    try:\n",
    "        # Try a common ID first\n",
    "        cookie_button_id = 'onetrust-accept-btn-handler' # This is a common ID\n",
    "        cookie_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.ID, cookie_button_id))\n",
    "        )\n",
    "        cookie_button.click()\n",
    "        print(\"Cookie banner (ID) likely accepted.\")\n",
    "        time.sleep(3) # Give time for banner to disappear\n",
    "    except Exception as e_cookie_id:\n",
    "        print(f\"Could not click cookie banner by ID '{cookie_button_id}': {e_cookie_id}\")\n",
    "        print(\"Continuing without explicit cookie banner click, hoping it doesn't interfere.\")\n",
    "\n",
    "\n",
    "    # Wait for the specific element containing the Fear & Greed value\n",
    "    # The HTML provided: <span class=\"market-fng-gauge__dial-number-value\">64</span>\n",
    "    target_value_class = \"market-fng-gauge__dial-number-value\"\n",
    "    WebDriverWait(driver, wait_time).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, target_value_class))\n",
    "    )\n",
    "    print(f\"Fear & Greed index value element (class: '{target_value_class}') seems to have loaded.\")\n",
    "\n",
    "    # Also wait for the timestamp element if desired\n",
    "    # The timestamp is usually in a div like: <div class=\"market-fng-gauge__timestamp\">Last updated ...</div>\n",
    "    timestamp_class = \"market-fng-gauge__timestamp\"\n",
    "    WebDriverWait(driver, wait_time).until(\n",
    "        EC.presence_of_element_located((By.CLASS_NAME, timestamp_class))\n",
    "    )\n",
    "    print(f\"Fear & Greed timestamp element (class: '{timestamp_class}') seems to have loaded.\")\n",
    "\n",
    "    time.sleep(2) # Extra pause for full rendering, especially if JS updates are slow\n",
    "\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # --- Extract Fear & Greed Index Value ---\n",
    "    fear_greed_index_str = \"N/A\" # Default value\n",
    "    value_span = soup.find('span', class_=target_value_class)\n",
    "    if value_span:\n",
    "        fear_greed_index_str = value_span.get_text(strip=True)\n",
    "        print(f\"Extracted Fear & Greed Index String: {fear_greed_index_str}\")\n",
    "        try:\n",
    "            data['fear_greed_index'] = int(fear_greed_index_str)\n",
    "        except ValueError:\n",
    "            data['fear_greed_index'] = fear_greed_index_str # Store as string if conversion fails\n",
    "            print(f\"Warning: Could not convert '{fear_greed_index_str}' to an integer.\")\n",
    "    else:\n",
    "        print(f\"Warning: Fear & Greed index value span not found using class '{target_value_class}'.\")\n",
    "        data['fear_greed_index'] = \"N/A\"\n",
    "    # --- Value Extraction End ---\n",
    "\n",
    "    # --- Extract Market Timestamp ---\n",
    "    market_timestamp_str = \"N/A\" # Default value\n",
    "    timestamp_div = soup.find('div', class_=timestamp_class)\n",
    "    if timestamp_div:\n",
    "        market_timestamp_str = timestamp_div.get_text(strip=True)\n",
    "        # Clean up the timestamp string if needed, e.g., remove \"Last updated \"\n",
    "        if \"Last updated \" in market_timestamp_str:\n",
    "            market_timestamp_str = market_timestamp_str.replace(\"Last updated \", \"\").strip()\n",
    "        print(f\"Extracted Market Timestamp: {market_timestamp_str}\")\n",
    "    else:\n",
    "        print(f\"Warning: Market timestamp div not found using class '{timestamp_class}'.\")\n",
    "\n",
    "    data['market_timestamp'] = market_timestamp_str # Add to data dictionary\n",
    "    # --- Timestamp Extraction End ---\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during scraping: {e}\")\n",
    "    if driver:\n",
    "        try:\n",
    "            # Save page source for debugging if an error occurs\n",
    "            debug_filename = \"cnn_error_page_debug.html\"\n",
    "            with open(debug_filename, \"w\", encoding=\"utf-8\") as f_debug:\n",
    "                f_debug.write(driver.page_source)\n",
    "            print(f\"Saved error page source to {debug_filename}\")\n",
    "        except Exception as e_save:\n",
    "            print(f\"Could not save debug page source: {e_save}\")\n",
    "\n",
    "finally:\n",
    "    if driver:\n",
    "        driver.quit()\n",
    "        print(\"Browser closed.\")\n",
    "\n",
    "print(\"\\n--- Extracted Data ---\")\n",
    "# Check if any meaningful data was extracted\n",
    "if data.get('fear_greed_index', \"N/A\") != \"N/A\" or data.get('market_timestamp', \"N/A\") != \"N/A\":\n",
    "    print(f\"Market Timestamp: {data.get('market_timestamp', 'N/A')}\")\n",
    "    print(f\"Fear & Greed Index: {data.get('fear_greed_index', 'N/A')}\")\n",
    "\n",
    "    # --- 将数据保存到 TXT 文件 (JSON 格式) ---\n",
    "    try:\n",
    "        with open(output_filename, 'w', encoding='utf-8') as f_out:\n",
    "            json.dump(data, f_out, indent=4, ensure_ascii=False)\n",
    "        print(f\"\\nData successfully saved to {output_filename}\")\n",
    "    except IOError as e_io:\n",
    "        print(f\"Error saving data to file {output_filename}: {e_io}\")\n",
    "    except Exception as e_json: # Catch any other exceptions during JSON processing\n",
    "        print(f\"Error  during JSON serialization or file writing: {e_json}\")\n",
    "    # --- 保存结束 ---\n",
    "else:\n",
    "    print(\"No significant data was extracted. Nothing to save to file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e8d7b6-c74e-4807-af7f-25a000ec029e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
